# üï∏Ô∏è WebScraper for AUPP and Related Sites

This project is an asynchronous web scraper built in Python that extracts English and Khmer content from structured web pages such as those from `https://www.aupp.edu.kh`. It automatically classifies text by language and saves the results to a CSV file for further analysis.

---

## üöÄ Features

- ‚úÖ **Asynchronous scraping** with `httpx` + `asyncio` for high performance
- üåê **HTTP/2 support** with custom headers and user-agent rotation
- üåç **Auto language detection** (English / Khmer) using `langdetect`
- üì§ **Clean CSV export** with deduplicated content
- üïì **Rate limiting** and retry logic for large scraping batches
- üìä **Logging system** for debugging and monitoring
- üéØ **Robots.txt compliance** checking

---

## ÔøΩ Project Structure

```
Web_Scrapping/
‚îú‚îÄ‚îÄ main.py                    # Main scraper script
‚îú‚îÄ‚îÄ get_cookies_playwright.py  # Cookie extraction utility
‚îú‚îÄ‚îÄ CrawlRobots.py            # Robots.txt checker
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ aupp.edu.kh_page_urls.txt # Sample URLs for testing
‚îú‚îÄ‚îÄ outputs/                  # Generated CSV files
‚îî‚îÄ‚îÄ scraper.log              # Application logs
```

---

## üì¶ Installation

### Prerequisites
- Python 3.13
- pip package manager

### Setup

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Kheav-Kienghok/WebScrapping.git
   cd WebScrapping
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

---

## üß™ Usage

### Basic Usage

Run the main scraper:
```bash
python main.py
```

The script will prompt you to enter URLs:
- Enter one URL per line
- Press Enter on an empty line to start scraping
- The scraper will process all URLs asynchronously

### Advanced Usage

**Check robots.txt compliance:**
```bash
python CrawlRobots.py
```

**Extract cookies using Playwright:**
```bash
python get_cookies_playwright.py
```

---

## üìä Output

### CSV Files
Scraped results are saved in timestamped CSV files in the `outputs/` directory:
```
outputs/scraped_content_YYYYMMDD_HHMMSS.csv
```

### CSV Structure
| Column | Description |
|--------|-------------|
| `ID` | Unique identifier for each scraped item |
| `English_Text` | Extracted English content |
| `Khmer_Text` | Extracted Khmer content |

> **Note:** Each row contains only one type of text for clean alignment and further processing.

### Logging
Application logs are saved to `scraper.log` with detailed information about:
- Scraping progress
- Error handling
- Rate limiting
- Language detection results

---

## ‚öôÔ∏è Configuration

### Rate Limiting
The scraper includes built-in rate limiting to respect target servers:
- Default delay between requests: 1-2 seconds
- Configurable retry attempts for failed requests
- Automatic backoff on rate limit responses

### Language Detection
- Uses `langdetect` library for automatic language classification
- Supports English and Khmer text detection
- Fallback handling for undetected languages

---

## üìã Requirements

Main dependencies:
- `httpx` - Async HTTP client
- `beautifulsoup4` - HTML parsing
- `langdetect` - Language detection
- `python-dotenv` - Environment variable management
- `playwright` - Browser automation (for cookie extraction)

For a complete list, see `requirements.txt`.

---

## ‚ö†Ô∏è Disclaimer

This tool is for educational and research purposes only. Please ensure you:
- Respect robots.txt files
- Follow website terms of service
- Use appropriate rate limiting
- Obtain necessary permissions before scraping